---
title: "Abacus API"
subtitle: "Search & Data Access"
author: "Mathew Vis-Dunbar"
date: "2024-11-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Set up

The Abacus API is accessible through http, which means we can access the API using nay compliant tool, including a web browser.

By default, the API returns data in JSON notation. One of the issues we have when working in `R` is that `R` excels with rectangular data, especially if working in the `Tidyverse`. The most challenging aspect of interacting with an API though `R` is, ostensibly, wranginlin JSON formatted  data into a data structure more amenbae to R. This begins by moving content into a list after which we can transition it to a dataframe, or tibble, as needed.

To do this all, we'll use the following four libraries:

```{r, load-libraries}
library(httr2) # http protocols
library(jsonlite) # json to r data structures
library(tidyr) # to create tidy data
library(dplyr) # to manipulate data
```

## Building the Connection

Abacus is built on a Dataverse instance. You can refer to the Dataverse API documentation for more information on interacting with the API. Abacus also has more than one API. Here, we'll review the `Search` and `Data Access` APIs; the former to locate content, the latter to retrieve that content.

In either case, we start with the base url into the API, which we'll store in a varible to later use.

```{r, base-url}
base_url <- "https://abacus.library.ubc.ca/api/"
```

After loading the base url, we can add on the specific API we intend to search.

```{r, api-branch}
search_api <- "search?q="
```

The `Search` API requires a `query` parameter, ie, something to search for. This is articulated as a string, folloing the `?q=`.

## Search API

There are a variety of fields and file types that we can search through the search API, noting that, as a data repository, objects in Abacus are organized into dataverses, datasets, and files, where datasets reside in dataverses, and files within datasets. Datasets provide a high level overview of the data they hold, including descriptions, number of files, who produced the files, when they were uploaded, etc.

For this demonstration, we'll focus on searching for datasets. We've already built our `base_url` and added on the requiste text to access the `Search` API, `search_api`. We'll no build our query, specifying the query and the file type of interest.

```{r, query}
search_query <- "title:labour" # retrieve everything with the word labour in the title
search_type <- "&type=dataset" # limit to datasets
```

We can then paste these together to construct a full query.

```{r, search-build}
(search_api_call <- paste0(base_url, search_api, search_query, search_type))
```

To get a sense of what this looks like, we can load this query in our browser.

## Implementing the Search in `R`

The `hhtr2` package will allow us to make http requests from within `R`. The steps invovled are building a request and then performing that request, which results in some data being sent back to us. These two steps are done, respectively, with the `request()` and `req_perform()` functions. We feed our `search_api_call` into the `request()` and then feed that into the `req_preform`.

```{r, generate-request}
search_resp <- search_api_call |>
  httr2::request() |>
  httr2::req_perform()
```

What we get back is exactly what we saw in our browser, formatted as a list.

```{r, explore-resp}
str(search_resp)
```

What wer're most interested in here is the `body`. However, the `body` is sent to us in raw bytes. This is geneally the case in any API response. `httr2` has a built in function for converting the body from raw to something `R` can work with, but we'll use the `jsonlite` package to explore this process step by step.

```{r, response-body}
summary(search_resp$body)
```

Since we only want the `body` going forward, we'll isolate this, and drop the other content.

```{r, isolate-body}
search_body <- search_resp$body
rm(search_resp)
```

## Data Wrangling

Our first task is convert this raw data to character data. We can do so with `rawToChar`.

```{r, raw-to-char}
search_body_char <- rawToChar(search_body)
```

We're presented with a vector of length one, which if we print out the first bit of, does not look very friendly. This is becuase this is in JSON notation.

```{r, search-body-char-summary}
cat("Summary\n")
summary(search_body_char)
cat("\nStructure\n")
str(search_body_char)
```

Our next task then, is to convert this JSON notation to a suitable `R` data structure, in this case, a list. We'll do this with the `jsonlite` package.

```{r, json-list-conversion}
search_body_list <- jsonlite::fromJSON(search_body_char)
```

If we look at hwo this has been converted, we can see it's a list of two.

```{r}
cat("Summary\n")
summary(search_body_list)
```

The `data` list is what we want. We'll pull that out and tidy up a bit, and then explore that list.

```{r, data-isolation}
search_data <- search_body_list$data
rm(list = ls(pattern = "^search_body"))
summary(search_data)
```

We're slowly wittling this down. This now looks almost identical to what we pulled up in our browser earlier. The actual relevant content is in the `items` list. But we'll also need the `total_count` for when we need to request all the data from the server in a loop. For now, let's look at the 26 items in there.

```{r, items-names}
names(search_data$items)
```

We don't need all this data. We should be able to do some evaluation with the `name`, `description`, `fileCount`, `producers`, `published_at`, `url`, and, for later retrieval, `global_id`. Let's look at these quickly, noting importantly, that `producers` is another list, that we'll need to wrangle some how.

```{r, vars-of-interest}
vars_of_interest <- c("name", "description", "fileCount", "producers", "published_at", "url", "global_id")
lapply(search_data$items[vars_of_interest], class)
```

## Getting All the Data

At this stage, we have all the information we need to pull these 5 variables for all datasets available on Abacus. A few things to note.

* By default, the `Search` API returns 10 results per request. It allows for a maximum of 1000 per request. This is handled in the `per_page` parameter.
* There is a start parameter that we can feed into the request; this indicates which record to start retrieving data from. This number will need to updated as we iterate over our requests, for example, if in the first call we pull records 1:1000, in the second call, we need to start at 1001. This is handled in the `start` paraemter.
* `R` starts with 1, other languages start at 0, so the first record returned by the API is indexed at 0, not 1.
* APIs will have a limit on the number of requests you can make in any given time span. We'll build in a simple delay using `Sys.Sleep()`, but `httr2` has a built in function for handling this as well.

I have built this into a function that we can call.

```{r, search-api-function}
search_datasets <- function(query, per_page) {
  # Build the query
  url <- paste0("https://abacus.library.ubc.ca/api/search?q=", query, "&type=dataset")
  
  # Initial call to figure out total records
  resp <- url |>
    httr2::request() |>
    httr2::req_perform()
  resp_body <- jsonlite::fromJSON(rawToChar(resp$body))
  total_count <- resp_body$data$total_count
  
  # Give some feedback
  cat(paste0("There are ", total_count, " records to be fetched.\n", "Fetching ", per_page, " records per call.\nThis will require ", ceiling(total_count/per_page), " calls.\n\n"))
  
  # build place holders for loop
  name <- vector()
  description <- vector()
  file_count <- vector()
  producers <- list()
  pub_date <- vector()
  global_id <- vector()
  handle <- vector()
  
  # establish the starting point
  start_point <- 0
  
  # create a counter for tracking calls
  call_counter <- 1
  
  # run the loop
  while(length(handle) < total_count) { # while the number of retrieved records is < the total count
    cat(paste0("Call ", call_counter, "\n")) # Indicate what call we're on to the API
    req <- httr2::request(paste0(url, "&start=", start_point, "&per_page=", per_page)) # create the request
    resp <- httr2::req_perform(req) # perform the request
    resp_body <- jsonlite::fromJSON(rawToChar(resp$body)) # convert the body from raw JSON to a list
    resp_body_name <- resp_body$data$items$name # get the name
    resp_body_desc <- resp_body$data$items$description # get the description
    resp_body_file_count <- resp_body$data$items$fileCount # get the file count
    resp_body_producers <- resp_body$data$items$producers # get the producers
    resp_body_handle <- resp_body$data$items$url #get the url
    resp_body_id <- resp_body$data$items$global_id # get id
    resp_body_pubdate <- resp_body$data$items$published_at # get update date
    # update the place holders:
    name <- c(name, resp_body_name)
    description <- c(description, resp_body_desc)
    file_count <- c(file_count, resp_body_file_count)
    producers <- c(producers, resp_body_producers)
    pub_date <- c(pub_date, resp_body_pubdate)
    handle <- c(handle, resp_body_handle)
    global_id <- c(global_id, resp_body_id)
    # update counters
    start_point <- start_point + per_page # increment the start_point
    call_counter <- call_counter + 1 # increment the counter
    Sys.sleep(.1) # pause before making a new call
  }
  # When all is said and done, compile the place holders into a list and return this object
    return(list("name" = name,
                "description" = description,
                "file_count" = file_count,
                "producers" = producers,
                "pub_date" = pub_date,
                "handle" = handle,
                "id" = global_id))
}
```

We can then call that function.

```{r, search-function-call}
labour <- search_datasets("title:labour", 50)
```

We now have a local copy that we can explore.

```{r, summary-all-datasets}
summary(labour)
```

We can make this a bit easier for ourselves by converting this into a dataframe or tibble. There are several ways to approach this, considering that `producers` is a list - it is a list as some records have more than one producer associated them. Here, I'll treat datasets with multiple producers as a different record authority than if one of these co-producers produced a standalone dataset. To do this, we'll first collapse the `producers` nested lists, and then flatten the result into a single list.

```{r, all-datasets-df-1}
labour$producers <- labour$producers |>
  lapply(function(x) paste(x, collapse = ", ")) |>
  unlist()

head(labour$producers, n = 20)
```

We can easily convert this to a tibble.

```{r, all-datasets-df-2}
labour <- tidyr::as_tibble(labour)
head(labour)
```

## Explore

We have several datasets with labour in the title. One way of handling exploring this is to strip the dates so that we can look for unique values. First we extract the date:

```{r}
labour$year <- gsub(".*?([0-9]+).*$", "\\1", labour$name)
head(labour$year)
```

Then we strip the name:

```{r}
labour$name_no_year <- trimws(gsub("[[:punct:]]|[[:digit:]]", "", labour$name))
head(labour$name_no_year)
```

And then look at the unique values:

```{r}
(unique(labour$name_no_year))
```

We only need the Labour Force Surveys themselves. These are occassionaly revised, [documentation on these revisions here](https://www150.statcan.gc.ca/n1/pub/71f0031x/71f0031x2023001-eng.htm), hence multiple records with slightly modified names (Labour Force Survey, Labour Force Survey (Rebased, Revised), etc)

```{r}
lfs <- labour[grepl("^Labour Force Survey", labour$name_no_year),]
head(lfs)
```

## Looking at Files

We now know the data sets we're interested in. A dataset is made up of multiple files, however, and we likely don't want all the files. We'll start by getting the file list for one of our datasets.

Up until now, we've been using the `search` api. We'll now use the `datasets` api.

```{r}
data_access_api <- "datasets/"
```

Like before, we'll build our querry. We'll start with the 2011 LFS dataset. This is a multistep process, as the first thing we need to do is get the id for the dataset, which is different from the `global_id` we retrieved from the `search` api, but we need the `global_id`, piped into the `datasets` api, to get the id for the dataset.

```{r}
id <- lfs[lfs$name == "Labour Force Survey, 2011", "id", drop = TRUE] # we want a simple vector returned
query <- paste0(":persistentId/?persistentId=", id)
(dataset_api_call <- paste0(base_url, data_access_api, query))
```

Now we make the call and process the `body`.

```{r}
dataset_id_resp <- dataset_api_call |>
  httr2::request() |>
  httr2::req_perform()
dataset_id_body <- jsonlite::fromJSON(rawToChar(dataset_id_resp$body))
```

We can now use the `id` and the `versionNumber` to access a file list.

First, build the query

```{r}
dataset_id <- dataset_id_body$data$id
dataset_ver <- dataset_id_body$data$latestVersion$versionNumber
(file_list_query <- paste0(base_url, data_access_api, dataset_id, "/versions/", dataset_ver, "/files/"))
```

Then execute, this time using the `httr2` option fro converting the body to an `R` object.

```{r}
dataset_file_resp <- file_list_query |>
  httr2::request() |>
  httr2::req_perform() |>
  httr2::resp_body_json()
```

All relevant data files have a `directoryLabel` equal to `Data`. We can use this to access only the data files.