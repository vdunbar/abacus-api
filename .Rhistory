for(i in data_files){
print(data_files[i])
}
for(i in 1:length(data_files)){
print(data_files[[i]])
}
for(i in 1:length(data_files)){
print(data_files[[i]]$label)
}
lapply(data_files, print(data_files$label))
lapply(data_files, print(x))
lapply(data_files, print(data_files))
lapply(data_files, print(data_files$label))
lapply(data_files, function(x) print(data_files[[x]]$label))
for(i in 1:length(data_files)){
print(data_files[[i]]$label)
}
relevant_data_files <- Filter(function(x), grep("tab$", data_files[[x]]$label), data_files)
relevant_data_files <- Filter(function(x) grep("tab$", data_files[[x]]$label), data_files)
grep("tab$", data_files[[1]])
grepl("tab$", data_files[[1]]$label)
relevant_data_files <- Filter(function(x) grepl("tab$", x[[1]]$label), data_files)
relevant_data_files <- Filter(function(x) grepl("tab$", x$label), data_files)
View(relevant_data_files)
relevant_data_files <- Filter(function(x) grepl("\.tab$", x$label), data_files)
relevant_data_files <- Filter(function(x) grepl("/.tab$", x$label), data_files)
relevant_data_files <- Filter(function(x) grepl("//.tab$", x$label), data_files)
relevant_data_files <- Filter(function(x) grepl(".tab$", x$label), data_files)
relevant_data_files <- Filter(function(x) grepl("tab$", x$label), data_files)
View(relevant_data_files)
length(relevant_data_files)
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_url <- relevant_data_files[[i]]$dataFile$pidURL
print(file_name)
print(file_url)
}
dest_dir <- dir.create("LFS_2011")
dir.create("LFS_2011")
dest_dir <- "LFS_2011/"
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_url <- relevant_data_files[[i]]$dataFile$pidURL
download.file(url = file_url, destfile = paste0(dest_dir, file_url))
}
dest_dir <- "LFS_2011/"
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_url <- relevant_data_files[[i]]$dataFile$pidURL
download.file(url = file_url, destfile = paste0(dest_dir, file_url))
}
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_url <- relevant_data_files[[i]]$dataFile$pidURL
download.file(url = file_url, destfile = paste0(dest_dir, file_name))
}
data <- read.delim("LFS_2011/LFS_April_2011.tab")
View(data)
View(data_files)
base_url
download_file <- "https://abacus.library.ubc.ca/api/access/datafile/:persistentId/?persistentId=hld:11272.1/AB2/GK3SFF/EUDSBO" |>
httr2::request() |>
httr2::req_perform()
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_id <- relevant_data_files[[i]]$dataFile$id
url_root <- "https://abacus.library.ubc.ca/api/access/datafile/"
download.file(url = paste0(url_root, file_id), destfile = paste0(dest_dir, file_name))
}
data <- read.delim("LFS_2011/LFS_April_2011.tab")
View(data)
list.files("LFS_2011/")
year_dat <- lapply(list.files("LFS_2011/"), function(x) read.delim(file = x))
year_dat <- lapply(list.files("LFS_2011/"), function(x) read.delim(file = paste0(dest_dir, x)))
year_df <- do.call(year_dat, rbind)
year_df <- do.call(rbind, year_dat)
View(year_df)
unique(year_df$SURVMNTH)
sort(unique(year_df$SURVMNTH))
lfs_2011_list <- lapply(list.files(dest_dir), function(x) read.delim(file = paste0(dest_dir, x)))
lfs_2011_df <- do.call(rbind, year_dat)
summary(lfs_2011_df)
---
title: "Abacus API"
knitr::opts_chunk$set(echo = TRUE)
library(httr2) # http protocols
library(jsonlite) # json to r data structures
base_url <- "https://abacus.library.ubc.ca/api/"
search_api <- "search?q="
search_query <- "title:labour" # retrieve everything with the word labour in the title
search_type <- "&type=dataset" # limit to datasets
(search_api_call <- paste0(base_url, search_api, search_query, search_type))
search_resp <- search_api_call |>
httr2::request() |>
httr2::req_perform()
str(search_resp)
summary(search_resp$body)
search_body <- search_resp$body
search_body <- search_resp$body
search_body_char <- rawToChar(search_body)
summary(search_body_char)
str(search_body_char)
summary(search_body_char)
str(search_body_char)
search_body_list <- jsonlite::fromJSON(search_body_char)
summary(search_body_list)
search_data <- search_body_list$data
summary(search_data)
names(search_data$items)
(search_body_list$data$total_count)
(nrow(search_body_list$data$items))
vars_of_interest <- c("name", "description", "fileCount", "producers", "published_at", "url", "global_id")
lapply(search_data$items[vars_of_interest], class) # show the class of each list item
(search_body_list$data$total_count)
(nrow(search_body_list$data$items))
search_datasets <- function(query, per_page) {
# Build the query
url <- paste0("https://abacus.library.ubc.ca/api/search?q=", query, "&type=dataset")
# Initial call to figure out total records
resp <- url |>
httr2::request() |>
httr2::req_perform()
resp_body <- jsonlite::fromJSON(rawToChar(resp$body))
total_count <- resp_body$data$total_count
# Give some feedback
cat(paste0("There are ", total_count, " records to be fetched.\n", "Fetching ", per_page, " records per call.\nThis will require ", ceiling(total_count/per_page), " calls.\n\n"))
# build place holders for loop
name <- vector()
description <- vector()
file_count <- vector()
producers <- list()
pub_date <- vector()
global_id <- vector()
handle <- vector()
# establish the starting point
start_point <- 0
# create a counter for tracking calls
call_counter <- 1
# run the loop
while(length(handle) < total_count) { # while the number of retrieved records is < the total count
cat(paste0("Call ", call_counter, "\n")) # Indicate what call we're on to the API
req <- httr2::request(paste0(url, "&start=", start_point, "&per_page=", per_page)) # create the request
resp <- httr2::req_perform(req) # perform the request
resp_body <- jsonlite::fromJSON(rawToChar(resp$body)) # convert the body from raw JSON to a list
resp_body_name <- resp_body$data$items$name # get the name
resp_body_desc <- resp_body$data$items$description # get the description
resp_body_file_count <- resp_body$data$items$fileCount # get the file count
resp_body_producers <- resp_body$data$items$producers # get the producers
resp_body_handle <- resp_body$data$items$url #get the url
resp_body_global_id <- resp_body$data$items$global_id # get id
resp_body_pubdate <- resp_body$data$items$published_at # get update date
# update the place holders:
name <- c(name, resp_body_name)
description <- c(description, resp_body_desc)
file_count <- c(file_count, resp_body_file_count)
producers <- c(producers, resp_body_producers)
pub_date <- c(pub_date, resp_body_pubdate)
handle <- c(handle, resp_body_handle)
global_id <- c(global_id, resp_body_global_id)
# update counters
start_point <- start_point + per_page # increment the start_point
call_counter <- call_counter + 1 # increment the counter
Sys.sleep(.1) # pause before making a new call
}
# When all is said and done, compile the place holders into a list and return this object
return(list("name" = name,
"description" = description,
"file_count" = file_count,
"producers" = producers,
"pub_date" = pub_date,
"handle" = handle,
"global_id" = global_id))
}
labour <- search_datasets("title:labour", 50)
summary(labour)
labour$producers <- labour$producers |>
lapply(function(x) paste(x, collapse = ", ")) |>
unlist()
head(labour$producers, n = 20)
labour <- as.data.frame(labour)
head(labour)
labour$year <- gsub(".*?([0-9]+).*$", "\\1", labour$name) # regex to find ending numbers and storing in a new var
head(labour$year)
labour$name_no_year <- trimws(gsub("[[:punct:]]|[[:digit:]]", "", labour$name)) # regex to remove punctuation and remaing numbers
head(labour$name_no_year)
(unique(labour$name_no_year))
lfs <- labour[grepl("^Labour Force Survey", labour$name_no_year),] # regex to find names starting with LFS
head(lfs)
datasets_api <- "datasets/"
id <- lfs[lfs$name == "Labour Force Survey, 2011", "global_id", drop = TRUE] # we want a simple vector returned
query <- paste0(":persistentId/?persistentId=", id) # limit to the id of interest, see dataverse API documentation
(dataset_api_call <- paste0(base_url, datasets_api, query)) # build the call
dataset_id_resp <- dataset_api_call |>
httr2::request() |>
httr2::req_perform()
dataset_id_body <- jsonlite::fromJSON(rawToChar(dataset_id_resp$body))
dataset_id <- dataset_id_body$data$id
dataset_ver <- dataset_id_body$data$latestVersion$versionNumber
(file_list_query <- paste0(base_url, datasets_api, dataset_id, "/versions/", dataset_ver, "/files/"))
dataset_file_resp <- file_list_query |>
httr2::request() |>
httr2::req_perform() |>
httr2::resp_body_json()
data_files <- Filter(function(x) x$directoryLabel == "Data", dataset_file_resp$data) # filter by directoryLabel
for(i in 1:length(data_files)){
print(data_files[[i]]$label)
}
relevant_data_files <- Filter(function(x) grepl("tab$", x$label), data_files) # filter to files ending in tab
dir.create("LFS_2011") # create a directory
dest_dir <- "LFS_2011/" # store it in a variable
# loop through the files and download them
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_id <- relevant_data_files[[i]]$dataFile$id
url_root <- "https://abacus.library.ubc.ca/api/access/datafile/"
download.file(url = paste0(url_root, file_id), destfile = paste0(dest_dir, file_name))
}
lfs_2011_list <- lapply(list.files(dest_dir), function(x) read.delim(file = paste0(dest_dir, x))) # read in each tab file into a list
lfs_2011_df <- do.call(rbind, lfs_2011_list) # bind the list items into a df
summary(lfs_2011_df)
install.packages("osmdata")
library(osmdata)
library(sf)
bc <- opq('British Columbia')
bc_sf <- osmdata_sp(bc)
bc <- opq('Kelowna, British Columbia')
bc_sf <- osmdata_sp(bc)
View(bc_sf)
bc <- opq('Kelowna, British Columbia') |>|
bc <- opq('Kelowna, British Columbia') |>|
bc <- opq('Kelowna, British Columbia') |>
add_osm_feature(key = "highway", value = "path") |>
osmdata_sp()
sp::plot(bc)
sp::plot(bc$osm_lines)
sp::plot(bc_sf$osm_lines)
knitr::opts_chunk$set(echo = TRUE)
library(httr2) # http protocols
library(jsonlite) # json to r data structures
base_url <- "https://abacus.library.ubc.ca/api/"
search_api <- "search?q="
search_query <- "title:labour" # retrieve everything with the word labour in the title
search_type <- "&type=dataset" # limit to datasets
(search_api_call <- paste0(base_url, search_api, search_query, search_type))
search_resp <- search_api_call |>
httr2::request() |>
httr2::req_perform()
str(search_resp)
summary(search_resp$body)
search_body <- search_resp$body
search_body_char <- rawToChar(search_body)
summary(search_body_char)
str(search_body_char)
cat(search_body_char)
?cat
print(search_body_char)
print(head(search_body_char))
?print
search_body_list <- jsonlite::fromJSON(search_body_char)
summary(search_body_list)
search_data <- search_body_list$data
summary(search_data)
search_data <- search_body_list$data
knitr::kable(summary(search_data))
search_data <- search_body_list$data
summary(search_data)
names(search_data$items)
vars_of_interest <- c("name", "description", "fileCount", "producers", "published_at", "url", "global_id")
lapply(search_data$items[vars_of_interest], class) # show the class of each list item
(search_body_list$data$total_count)
(nrow(search_body_list$data$items))
search_datasets <- function(query, per_page) {
# Build the query
url <- paste0("https://abacus.library.ubc.ca/api/search?q=", query, "&type=dataset")
# Initial call to figure out total records
resp <- url |>
httr2::request() |>
httr2::req_perform()
resp_body <- jsonlite::fromJSON(rawToChar(resp$body))
total_count <- resp_body$data$total_count
# Give some feedback
cat(paste0("There are ", total_count, " records to be fetched.\n", "Fetching ", per_page, " records per call.\nThis will require ", ceiling(total_count/per_page), " calls.\n\n"))
# build place holders for loop
name <- vector()
description <- vector()
file_count <- vector()
producers <- list()
pub_date <- vector()
global_id <- vector()
handle <- vector()
# establish the starting point
start_point <- 0
# create a counter for tracking calls
call_counter <- 1
# run the loop
while(length(handle) < total_count) { # while the number of retrieved records is < the total count
cat(paste0("Call ", call_counter, "\n")) # Indicate what call we're on to the API
req <- httr2::request(paste0(url, "&start=", start_point, "&per_page=", per_page)) # create the request
resp <- httr2::req_perform(req) # perform the request
resp_body <- jsonlite::fromJSON(rawToChar(resp$body)) # convert the body from raw JSON to a list
resp_body_name <- resp_body$data$items$name # get the name
resp_body_desc <- resp_body$data$items$description # get the description
resp_body_file_count <- resp_body$data$items$fileCount # get the file count
resp_body_producers <- resp_body$data$items$producers # get the producers
resp_body_handle <- resp_body$data$items$url #get the url
resp_body_global_id <- resp_body$data$items$global_id # get id
resp_body_pubdate <- resp_body$data$items$published_at # get update date
# update the place holders:
name <- c(name, resp_body_name)
description <- c(description, resp_body_desc)
file_count <- c(file_count, resp_body_file_count)
producers <- c(producers, resp_body_producers)
pub_date <- c(pub_date, resp_body_pubdate)
handle <- c(handle, resp_body_handle)
global_id <- c(global_id, resp_body_global_id)
# update counters
start_point <- start_point + per_page # increment the start_point
call_counter <- call_counter + 1 # increment the counter
Sys.sleep(.1) # pause before making a new call
}
# When all is said and done, compile the place holders into a list and return this object
return(list("name" = name,
"description" = description,
"file_count" = file_count,
"producers" = producers,
"pub_date" = pub_date,
"handle" = handle,
"global_id" = global_id))
}
labour <- search_datasets("title:labour", 50)
summary(labour)
knitr::kable(summary(labour))
labour$producers <- labour$producers |>
lapply(function(x) paste(x, collapse = ", ")) |>
unlist()
head(labour$producers, n = 20)
labour <- as.data.frame(labour)
knitr::kable(head(labour))
labour$year <- gsub(".*?([0-9]+).*$", "\\1", labour$name) # regex to find ending numbers and storing in a new var
head(labour$year)
labour$name_no_year <- trimws(gsub("[[:punct:]]|[[:digit:]]", "", labour$name)) # regex to remove punctuation and remaing numbers
head(labour$name_no_year)
(unique(labour$name_no_year))
lfs <- labour[grepl("^Labour Force Survey", labour$name_no_year),] # regex to find names starting with LFS
head(lfs)
knitr::kable(head(lfs))
lfs <- labour[grepl("^Labour Force Survey", labour$name_no_year),] # regex to find names starting with LFS
head(lfs)
lfs <- labour[grepl("^Labour Force Survey", labour$name_no_year),] # regex to find names starting with LFS
head(lfs)
lfs <- labour[grepl("^Labour Force Survey", labour$name_no_year),] # regex to find names starting with LFS
head(lfs)
datasets_api <- "datasets/"
id <- lfs[lfs$name == "Labour Force Survey, 2011", "global_id", drop = TRUE] # we want a simple vector returned
query <- paste0(":persistentId/?persistentId=", id) # limit to the id of interest, see dataverse API documentation
(dataset_api_call <- paste0(base_url, datasets_api, query)) # build the call
dataset_id_resp <- dataset_api_call |>
httr2::request() |>
httr2::req_perform()
dataset_id_body <- jsonlite::fromJSON(rawToChar(dataset_id_resp$body))
dataset_id <- dataset_id_body$data$id
dataset_ver <- dataset_id_body$data$latestVersion$versionNumber
(file_list_query <- paste0(base_url, datasets_api, dataset_id, "/versions/", dataset_ver, "/files/"))
dataset_file_resp <- file_list_query |>
httr2::request() |>
httr2::req_perform() |>
httr2::resp_body_json()
data_files <- Filter(function(x) x$directoryLabel == "Data", dataset_file_resp$data) # filter by directoryLabel
for(i in 1:length(data_files)){
print(data_files[[i]]$label)
}
relevant_data_files <- Filter(function(x) grepl("tab$", x$label), data_files) # filter to files ending in tab
dir.create("LFS_2011") # create a directory
dest_dir <- "LFS_2011/" # store it in a variable
# loop through the files and download them
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_id <- relevant_data_files[[i]]$dataFile$id
url_root <- "https://abacus.library.ubc.ca/api/access/datafile/"
download.file(url = paste0(url_root, file_id), destfile = paste0(dest_dir, file_name))
}
lfs_2011_list <- lapply(list.files(dest_dir), function(x) read.delim(file = paste0(dest_dir, x))) # read in each tab file into a list
lfs_2011_df <- do.call(rbind, lfs_2011_list) # bind the list items into a df
summary(lfs_2011_df)
View(lfs_2011_df)
knitr::kable(summary(search_data, "pipe"))
knitr::kable(summary(search_data, "pipe", align = "r"))
knitr::kable(summary(search_data, "pipe", align = "c"))
knitr::kable(summary(search_data, "pipe", align = "l"))
library(kableExtra)
summary(search_data) |>
kbl() |>
kable_styling()
#knitr::kable(summary(search_data, "pipe", align = "l"))
summary(search_data) |>
kbl() |>
kable_minimal()
summary(search_data) |>
kbl() |>
kable_material_dark()
summary(search_data) |>
kbl() |>
kable_material_dark() |>
kable_styling(bootstrap_options = "striped")
summary(search_data) |>
kbl() |>
#kable_material_dark() |>
kable_styling(bootstrap_options = "striped")
summary(labour) |>
kbl() |>
kable_styling(bootstrap_options = "striped")
summary(labour) |>
kbl()
summary(labour)
str(labour)
knitr::kable(summary(labour))
summary(labour) |>
kbl() |>
kable_styling(bootstrap_options = "striped")
head(labour) |>
kbl() |>
kable_styling(bootstrap_options = "striped")
head(labour) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(2, width = "25%")
head(labour) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(2, width = "50%")
head(labour) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(2, width = "5em")
head(labour) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(2, width = "30em")
head(labour) |>
kbl() |>
kable_paper(full_width = T) |>
column_spec(2, width = "30em")
head(labour) |>
kbl() |>
kable_paper(full_width = T) |>
column_spec(2, width = "50em")
head(labour) |>
kbl() |>
kable_paper(full_width = T) |>
column_spec(2, width = "50em", background = "yellow")
head(labour) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(2, width = "30em", background = "blue")
head(labour) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(2, width = "5%")
head(labour) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(2, width = "50%")
head(labour) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(1:7, width = "5%")
labour_trun <- labour
labour_trun$description <- substr(labour_trun$description, start = 1, stop = 30)
head(labour_trun) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(1:7, width = "5%")
labour_trun <- labour
labour_trun$description <- substr(labour_trun$description, start = 1, stop = 45)
head(labour_trun) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(1:7, width = "5%")
labour_trun <- labour
labour_trun$description <- substr(labour_trun$description, start = 1, stop = 45)
labour_trun$description <- paste0(labour_trun$description, "...")
head(labour_trun) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(1:7, width = "5%")
labour_trun <- labour
labour_trun$description <- substr(labour_trun$description, start = 1, stop = 44)
labour_trun$description <- paste0(labour_trun$description, "...")
head(labour_trun) |>
kbl() |>
kable_styling(bootstrap_options = "striped") |>
column_spec(1:7, width = "5%")
labour_trun <- labour
labour_trun$description <- substr(labour_trun$description, start = 1, stop = 44)
labour_trun$description <- paste0(labour_trun$description, "...")
head(labour_trun) |>
kbl() |>
kable_styling(bootstrap_options = "striped")
lfs_trun <- lfs
lfs_trun$description <- substr(lfs_trun$description, start = 1, stop = 44)
lfs_trun$description <- paste0(lfs_trun$description, "...")
head(lfs_trun) |>
kbl() |>
kable_styling(bootstrap_options = "striped")
summary(lfs_2011_df) |>
kbl()
summary(lfs_2011_df) |>
kbl() |>
kable_styling(bootstrap_options = "striped")
summary(lfs_2011_df[, c(1:11)]) |>
kbl() |>
kable_styling(bootstrap_options = "striped")
