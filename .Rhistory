file_resp <- req |>
httr2::request() |>
httr2::req_perform()
search_datasets <- function(query, per_page) {
# Build the query
url <- paste0("https://abacus.library.ubc.ca/api/search?q=", query, "&type=dataset")
# Initial call to figure out total records
resp <- url |>
httr2::request() |>
httr2::req_perform()
resp_body <- jsonlite::fromJSON(rawToChar(resp$body))
total_count <- resp_body$data$total_count
# Give some feedback
cat(paste0("There are ", total_count, " records to be fetched.\n", "Fetching ", per_page, " records per call.\nThis will require ", ceiling(total_count/per_page), " calls.\n\n"))
# build place holders for loop
name <- vector()
description <- vector()
file_count <- vector()
producers <- list()
pub_date <- vector()
global_id <- vector()
handle <- vector()
# establish the starting point
start_point <- 0
# create a counter for tracking calls
call_counter <- 1
# run the loop
while(length(handle) < total_count) { # while the number of retrieved records is < the total count
cat(paste0("Call ", call_counter, "\n")) # Indicate what call we're on to the API
req <- httr2::request(paste0(url, "&start=", start_point, "&per_page=", per_page)) # create the request
resp <- httr2::req_perform(req) # perform the request
resp_body <- jsonlite::fromJSON(rawToChar(resp$body)) # convert the body from raw JSON to a list
resp_body_name <- resp_body$data$items$name # get the name
resp_body_desc <- resp_body$data$items$description # get the description
resp_body_file_count <- resp_body$data$items$fileCount # get the file count
resp_body_producers <- resp_body$data$items$producers # get the producers
resp_body_handle <- resp_body$data$items$url #get the url
resp_body_id <- resp_body$data$items$global_id # get id
resp_body_pubdate <- resp_body$data$items$published_at # get update date
# update the place holders:
name <- c(name, resp_body_name)
description <- c(description, resp_body_desc)
file_count <- c(file_count, resp_body_file_count)
producers <- c(producers, resp_body_producers)
pub_date <- c(pub_date, resp_body_pubdate)
handle <- c(handle, resp_body_handle)
global_id <- c(global_id, resp_body_id)
# update counters
start_point <- start_point + per_page # increment the start_point
call_counter <- call_counter + 1 # increment the counter
Sys.sleep(.1) # pause before making a new call
}
# When all is said and done, compile the place holders into a list and return this object
return(list("name" = name,
"description" = description,
"file_count" = file_count,
"producers" = producers,
"pub_date" = pub_date,
"handle" = handle,
"id" = global_id))
}
labour <- search_datasets("title:labour", 50)
summary(labour)
labour$producers <- labour$producers |>
lapply(function(x) paste(x, collapse = ", ")) |>
unlist()
head(labour$producers, n = 20)
labour <- tidyr::as_tibble(labour)
head(labour)
labour$year <- gsub(".*?([0-9]+).*$", "\\1", labour$name)
head(labour$year)
labour$name_no_year <- trimws(gsub("[[:punct:]]|[[:digit:]]", "", labour$name))
head(labour$name_no_year)
(unique(labour$name_no_year))
lfs <- labour[grepl("^Labour Force Survey", labour$name_no_year),]
head(lfs)
View(lfs)
id <- lfs[lfs$name == "Labour Force Survey, 2011",]
id <- lfs[lfs$name == "Labour Force Survey, 2011", "id"]
id <- lfs[[lfs$name == "Labour Force Survey, 2011", "id"]]
id <- lfs[lfs$name == "Labour Force Survey, 2011", "id", drop = TRUE]
id <- lfs[lfs$name == "Labour Force Survey, 2011", "id", drop = TRUE] # we want a simple vector returned
query <- paste0("persistentId/?persistentId=", id)
dataset_api_call <- paste0(base_url, data_access_api, query)
dataset_id_resp <- dataset_api_call |>
httr2::request() |>
httr2::req_perform()
id <- lfs[lfs$name == "Labour Force Survey, 2011", "id", drop = TRUE] # we want a simple vector returned
query <- paste0("persistentId/?persistentId=", id)
(dataset_api_call <- paste0(base_url, data_access_api, query))
id <- lfs[lfs$name == "Labour Force Survey, 2011", "id", drop = TRUE] # we want a simple vector returned
query <- paste0(":persistentId/?persistentId=", id)
(dataset_api_call <- paste0(base_url, data_access_api, query))
dataset_id_resp <- dataset_api_call |>
httr2::request() |>
httr2::req_perform()
data_access_api <- "datasets/"
id <- lfs[lfs$name == "Labour Force Survey, 2011", "id", drop = TRUE] # we want a simple vector returned
query <- paste0(":persistentId/?persistentId=", id)
(dataset_api_call <- paste0(base_url, data_access_api, query))
dataset_id_resp <- dataset_api_call |>
httr2::request() |>
httr2::req_perform()
dataset_id_body <- jsonlite::fromJSON(rawToChar(dataset_id_resp$body))
dataset_file_resp <- file_list_query |>
httr2::request() |>
httr2::req_perform() |>
httr2::req_body_json()
dataset_id <- dataset_id_body$data$id
dataset_ver <- dataset_id_body$data$latestVersion$versionNumber
file_list_query <- paste0(data_access_api, dataset_id, "/version/", dataset_ver, "/files/")
dataset_file_resp <- file_list_query |>
httr2::request() |>
httr2::req_perform() |>
httr2::req_body_json()
dataset_id <- dataset_id_body$data$id
dataset_ver <- dataset_id_body$data$latestVersion$versionNumber
(file_list_query <- paste0(data_access_api, dataset_id, "/version/", dataset_ver, "/files/"))
dataset_id <- dataset_id_body$data$id
dataset_ver <- dataset_id_body$data$latestVersion$versionNumber
(file_list_query <- paste0(base_url, data_access_api, dataset_id, "/version/", dataset_ver, "/files/"))
dataset_file_resp <- file_list_query |>
httr2::request() |>
httr2::req_perform() |>
httr2::req_body_json()
dataset_id <- dataset_id_body$data$id
dataset_ver <- dataset_id_body$data$latestVersion$versionNumber
(file_list_query <- paste0(base_url, data_access_api, dataset_id, "/versions/", dataset_ver, "/files/"))
dataset_file_resp <- file_list_query |>
httr2::request() |>
httr2::req_perform() |>
httr2::req_body_json()
dataset_file_resp <- file_list_query |>
httr2::request() |>
httr2::req_perform() |>
httr2::resp_body_json()
View(dataset_file_resp)
knitr::opts_chunk$set(echo = TRUE)
library(httr2) # http protocols
library(jsonlite) # json to r data structures
library(tidyr) # to create tidy data
library(dplyr) # to manipulate data
base_url <- "https://abacus.library.ubc.ca/api/"
search_api <- "search?q="
search_query <- "title:labour" # retrieve everything with the word labour in the title
search_type <- "&type=dataset" # limit to datasets
(search_api_call <- paste0(base_url, search_api, search_query, search_type))
search_resp <- search_api_call |>
httr2::request() |>
httr2::req_perform()
summary(search_resp$body)
search_body <- search_resp$body
search_body_char <- rawToChar(search_body)
cat("Summary\n")
summary(search_body_char)
cat("\nStructure\n")
str(search_body_char)
search_body_list <- jsonlite::fromJSON(search_body_char)
summary(search_body_list)
search_data <- search_body_list$data
summary(search_data)
vars_of_interest <- c("name", "description", "fileCount", "producers", "published_at", "url", "global_id")
lapply(search_data$items[vars_of_interest], class) # show the class of each list item
(search_body_list$data$total_count)
(str(search_body_list$data$items))
(search_body_list$data$total_count)
(nrow(search_body_list$data$items))
search_datasets <- function(query, per_page) {
# Build the query
url <- paste0("https://abacus.library.ubc.ca/api/search?q=", query, "&type=dataset")
# Initial call to figure out total records
resp <- url |>
httr2::request() |>
httr2::req_perform()
resp_body <- jsonlite::fromJSON(rawToChar(resp$body))
total_count <- resp_body$data$total_count
# Give some feedback
cat(paste0("There are ", total_count, " records to be fetched.\n", "Fetching ", per_page, " records per call.\nThis will require ", ceiling(total_count/per_page), " calls.\n\n"))
# build place holders for loop
name <- vector()
description <- vector()
file_count <- vector()
producers <- list()
pub_date <- vector()
global_id <- vector()
handle <- vector()
# establish the starting point
start_point <- 0
# create a counter for tracking calls
call_counter <- 1
# run the loop
while(length(handle) < total_count) { # while the number of retrieved records is < the total count
cat(paste0("Call ", call_counter, "\n")) # Indicate what call we're on to the API
req <- httr2::request(paste0(url, "&start=", start_point, "&per_page=", per_page)) # create the request
resp <- httr2::req_perform(req) # perform the request
resp_body <- jsonlite::fromJSON(rawToChar(resp$body)) # convert the body from raw JSON to a list
resp_body_name <- resp_body$data$items$name # get the name
resp_body_desc <- resp_body$data$items$description # get the description
resp_body_file_count <- resp_body$data$items$fileCount # get the file count
resp_body_producers <- resp_body$data$items$producers # get the producers
resp_body_handle <- resp_body$data$items$url #get the url
resp_body_id <- resp_body$data$items$global_id # get id
resp_body_pubdate <- resp_body$data$items$published_at # get update date
# update the place holders:
name <- c(name, resp_body_name)
description <- c(description, resp_body_desc)
file_count <- c(file_count, resp_body_file_count)
producers <- c(producers, resp_body_producers)
pub_date <- c(pub_date, resp_body_pubdate)
handle <- c(handle, resp_body_handle)
global_id <- c(global_id, resp_body_id)
# update counters
start_point <- start_point + per_page # increment the start_point
call_counter <- call_counter + 1 # increment the counter
Sys.sleep(.1) # pause before making a new call
}
# When all is said and done, compile the place holders into a list and return this object
return(list("name" = name,
"description" = description,
"file_count" = file_count,
"producers" = producers,
"pub_date" = pub_date,
"handle" = handle,
"id" = global_id))
}
labour <- search_datasets("title:labour", 50)
summary(labour)
labour$producers <- labour$producers |>
lapply(function(x) paste(x, collapse = ", ")) |>
unlist()
head(labour$producers, n = 20)
labour <- tidyr::as_tibble(labour)
head(labour)
labour$year <- gsub(".*?([0-9]+).*$", "\\1", labour$name)
head(labour$year)
labour$name_no_year <- trimws(gsub("[[:punct:]]|[[:digit:]]", "", labour$name))
head(labour$name_no_year)
(unique(labour$name_no_year))
lfs <- labour[grepl("^Labour Force Survey", labour$name_no_year),]
head(lfs)
data_access_api <- "datasets/"
search_datasets <- function(query, per_page) {
# Build the query
url <- paste0("https://abacus.library.ubc.ca/api/search?q=", query, "&type=dataset")
# Initial call to figure out total records
resp <- url |>
httr2::request() |>
httr2::req_perform()
resp_body <- jsonlite::fromJSON(rawToChar(resp$body))
total_count <- resp_body$data$total_count
# Give some feedback
cat(paste0("There are ", total_count, " records to be fetched.\n", "Fetching ", per_page, " records per call.\nThis will require ", ceiling(total_count/per_page), " calls.\n\n"))
# build place holders for loop
name <- vector()
description <- vector()
file_count <- vector()
producers <- list()
pub_date <- vector()
global_id <- vector()
handle <- vector()
# establish the starting point
start_point <- 0
# create a counter for tracking calls
call_counter <- 1
# run the loop
while(length(handle) < total_count) { # while the number of retrieved records is < the total count
cat(paste0("Call ", call_counter, "\n")) # Indicate what call we're on to the API
req <- httr2::request(paste0(url, "&start=", start_point, "&per_page=", per_page)) # create the request
resp <- httr2::req_perform(req) # perform the request
resp_body <- jsonlite::fromJSON(rawToChar(resp$body)) # convert the body from raw JSON to a list
resp_body_name <- resp_body$data$items$name # get the name
resp_body_desc <- resp_body$data$items$description # get the description
resp_body_file_count <- resp_body$data$items$fileCount # get the file count
resp_body_producers <- resp_body$data$items$producers # get the producers
resp_body_handle <- resp_body$data$items$url #get the url
resp_body_global_id <- resp_body$data$items$global_id # get id
resp_body_pubdate <- resp_body$data$items$published_at # get update date
# update the place holders:
name <- c(name, resp_body_name)
description <- c(description, resp_body_desc)
file_count <- c(file_count, resp_body_file_count)
producers <- c(producers, resp_body_producers)
pub_date <- c(pub_date, resp_body_pubdate)
handle <- c(handle, resp_body_handle)
global_id <- c(global_id, resp_body_id)
# update counters
start_point <- start_point + per_page # increment the start_point
call_counter <- call_counter + 1 # increment the counter
Sys.sleep(.1) # pause before making a new call
}
# When all is said and done, compile the place holders into a list and return this object
return(list("name" = name,
"description" = description,
"file_count" = file_count,
"producers" = producers,
"pub_date" = pub_date,
"handle" = handle,
"global_id" = global_id))
}
labour <- search_datasets("title:labour", 50)
search_datasets <- function(query, per_page) {
# Build the query
url <- paste0("https://abacus.library.ubc.ca/api/search?q=", query, "&type=dataset")
# Initial call to figure out total records
resp <- url |>
httr2::request() |>
httr2::req_perform()
resp_body <- jsonlite::fromJSON(rawToChar(resp$body))
total_count <- resp_body$data$total_count
# Give some feedback
cat(paste0("There are ", total_count, " records to be fetched.\n", "Fetching ", per_page, " records per call.\nThis will require ", ceiling(total_count/per_page), " calls.\n\n"))
# build place holders for loop
name <- vector()
description <- vector()
file_count <- vector()
producers <- list()
pub_date <- vector()
global_id <- vector()
handle <- vector()
# establish the starting point
start_point <- 0
# create a counter for tracking calls
call_counter <- 1
# run the loop
while(length(handle) < total_count) { # while the number of retrieved records is < the total count
cat(paste0("Call ", call_counter, "\n")) # Indicate what call we're on to the API
req <- httr2::request(paste0(url, "&start=", start_point, "&per_page=", per_page)) # create the request
resp <- httr2::req_perform(req) # perform the request
resp_body <- jsonlite::fromJSON(rawToChar(resp$body)) # convert the body from raw JSON to a list
resp_body_name <- resp_body$data$items$name # get the name
resp_body_desc <- resp_body$data$items$description # get the description
resp_body_file_count <- resp_body$data$items$fileCount # get the file count
resp_body_producers <- resp_body$data$items$producers # get the producers
resp_body_handle <- resp_body$data$items$url #get the url
resp_body_global_id <- resp_body$data$items$global_id # get id
resp_body_pubdate <- resp_body$data$items$published_at # get update date
# update the place holders:
name <- c(name, resp_body_name)
description <- c(description, resp_body_desc)
file_count <- c(file_count, resp_body_file_count)
producers <- c(producers, resp_body_producers)
pub_date <- c(pub_date, resp_body_pubdate)
handle <- c(handle, resp_body_handle)
global_id <- c(global_id, resp_body_global_id)
# update counters
start_point <- start_point + per_page # increment the start_point
call_counter <- call_counter + 1 # increment the counter
Sys.sleep(.1) # pause before making a new call
}
# When all is said and done, compile the place holders into a list and return this object
return(list("name" = name,
"description" = description,
"file_count" = file_count,
"producers" = producers,
"pub_date" = pub_date,
"handle" = handle,
"global_id" = global_id))
}
labour <- search_datasets("title:labour", 50)
labour$producers <- labour$producers |>
lapply(function(x) paste(x, collapse = ", ")) |>
unlist()
head(labour$producers, n = 20)
labour <- tidyr::as_tibble(labour)
head(labour)
labour$year <- gsub(".*?([0-9]+).*$", "\\1", labour$name)
head(labour$year)
labour$name_no_year <- trimws(gsub("[[:punct:]]|[[:digit:]]", "", labour$name))
head(labour$name_no_year)
lfs <- labour[grepl("^Labour Force Survey", labour$name_no_year),]
head(lfs)
id <- lfs[lfs$name == "Labour Force Survey, 2011", "global_id", drop = TRUE] # we want a simple vector returned
query <- paste0(":persistentId/?persistentId=", id)
(dataset_api_call <- paste0(base_url, data_access_api, query))
id <- lfs[lfs$name == "Labour Force Survey, 2011", "global_id", drop = TRUE] # we want a simple vector returned
query <- paste0(":persistentId/?persistentId=", id) # limit to the id of interest
(dataset_api_call <- paste0(base_url, data_access_api, query)) # build the call
dataset_id_resp <- dataset_api_call |>
httr2::request() |>
httr2::req_perform()
dataset_id_body <- jsonlite::fromJSON(rawToChar(dataset_id_resp$body))
dataset_id <- dataset_id_body$data$id
dataset_ver <- dataset_id_body$data$latestVersion$versionNumber
(file_list_query <- paste0(base_url, data_access_api, dataset_id, "/versions/", dataset_ver, "/files/"))
dataset_file_resp <- file_list_query |>
httr2::request() |>
httr2::req_perform() |>
httr2::resp_body_json()
View(dataset_file_resp)
?Filter
data_files <- Filter(function(x) x$directoryLabel == "Data")
data_files <- Filter(function(x) x$directoryLabel == "Data", dataset_file_resp)
data_files <- lapply(dataset_file_resp, funciont(x) subset(dataset_file_resp(x), directoryLabel == "Data"))
data_files <- lapply(dataset_file_resp, funciont(x) x + 1)
data_files <- lapply(dataset_file_resp, funciont(x) print(x))
data_files <- lapply(dataset_file_resp, funciont(x) print(dataset_file_resp$x))
data_files <- lapply(dataset_file_resp, funciont(x) print(dataset_file_resp[x]))
dataset_file_resp[1]
dataset_file_resp$data[1]
data_files <- lapply(dataset_file_resp$data, funciont(x) print(dataset_file_resp$directoryLabel))
dataset_file_resp$data$directoryLabel
dataset_file_resp$data[1]directoryLabel
dataset_file_resp$data[1]$directoryLabel
dataset_file_resp$data[[1]]$directoryLabel
lapply(dataset_file_resp$data, funciont(x) print(x[[1]]$directoryLabel))
lapply(dataset_file_resp$data, funciont(x) x[[1]]$directoryLabel)
lapply(dataset_file_resp$data, print(x[[1]]$directoryLabel))
lapply(dataset_file_resp$data, print(dataset_file_resp$data[[1]]$directoryLabel)
)
print("hi")
df <- lapply(dataset_file_resp$data, subset(directoryLabel))
Filter(function(x) directoryLabel == "Data", dataset_file_resp$data)
Filter(function(x) x$directoryLabel == "Data", dataset_file_resp$data)
data_files <- Filter(function(x) x$directoryLabel == "Data", dataset_file_resp$data)
View(data_files)
data_files <- Filter(function(x) x$directoryLabel == "Data", dataset_file_resp$data)
sapply(data_files, print(data_files$label))
sapply(data_files, function(x) print(data_files[[x]]$label))
lapply(data_files, function(x) print(data_files[[x]]$label))
for(i in data_files){
print(data_files[[i]]$label)
}
for(i in data_files){
print(i)
}
for(i in data_files){
print(data_files[[i]])
}
for(i in data_files){
print(data_files[i)
for(i in data_files){
print(data_files[i])
}
for(i in 1:length(data_files)){
print(data_files[[i]])
}
for(i in 1:length(data_files)){
print(data_files[[i]]$label)
}
lapply(data_files, print(data_files$label))
lapply(data_files, print(x))
lapply(data_files, print(data_files))
lapply(data_files, print(data_files$label))
lapply(data_files, function(x) print(data_files[[x]]$label))
for(i in 1:length(data_files)){
print(data_files[[i]]$label)
}
relevant_data_files <- Filter(function(x), grep("tab$", data_files[[x]]$label), data_files)
relevant_data_files <- Filter(function(x) grep("tab$", data_files[[x]]$label), data_files)
grep("tab$", data_files[[1]])
grepl("tab$", data_files[[1]]$label)
relevant_data_files <- Filter(function(x) grepl("tab$", x[[1]]$label), data_files)
relevant_data_files <- Filter(function(x) grepl("tab$", x$label), data_files)
View(relevant_data_files)
relevant_data_files <- Filter(function(x) grepl("\.tab$", x$label), data_files)
relevant_data_files <- Filter(function(x) grepl("/.tab$", x$label), data_files)
relevant_data_files <- Filter(function(x) grepl("//.tab$", x$label), data_files)
relevant_data_files <- Filter(function(x) grepl(".tab$", x$label), data_files)
relevant_data_files <- Filter(function(x) grepl("tab$", x$label), data_files)
View(relevant_data_files)
length(relevant_data_files)
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_url <- relevant_data_files[[i]]$dataFile$pidURL
print(file_name)
print(file_url)
}
dest_dir <- dir.create("LFS_2011")
dir.create("LFS_2011")
dest_dir <- "LFS_2011/"
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_url <- relevant_data_files[[i]]$dataFile$pidURL
download.file(url = file_url, destfile = paste0(dest_dir, file_url))
}
dest_dir <- "LFS_2011/"
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_url <- relevant_data_files[[i]]$dataFile$pidURL
download.file(url = file_url, destfile = paste0(dest_dir, file_url))
}
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_url <- relevant_data_files[[i]]$dataFile$pidURL
download.file(url = file_url, destfile = paste0(dest_dir, file_name))
}
data <- read.delim("LFS_2011/LFS_April_2011.tab")
View(data)
View(data_files)
base_url
download_file <- "https://abacus.library.ubc.ca/api/access/datafile/:persistentId/?persistentId=hld:11272.1/AB2/GK3SFF/EUDSBO" |>
httr2::request() |>
httr2::req_perform()
for(i in 1:length(relevant_data_files)){
file_name <- relevant_data_files[[i]]$label
file_id <- relevant_data_files[[i]]$dataFile$id
url_root <- "https://abacus.library.ubc.ca/api/access/datafile/"
download.file(url = paste0(url_root, file_id), destfile = paste0(dest_dir, file_name))
}
data <- read.delim("LFS_2011/LFS_April_2011.tab")
View(data)
list.files("LFS_2011/")
year_dat <- lapply(list.files("LFS_2011/"), function(x) read.delim(file = x))
year_dat <- lapply(list.files("LFS_2011/"), function(x) read.delim(file = paste0(dest_dir, x)))
year_df <- do.call(year_dat, rbind)
year_df <- do.call(rbind, year_dat)
View(year_df)
unique(year_df$SURVMNTH)
sort(unique(year_df$SURVMNTH))
lfs_2011_list <- lapply(list.files(dest_dir), function(x) read.delim(file = paste0(dest_dir, x)))
lfs_2011_df <- do.call(rbind, year_dat)
summary(lfs_2011_df)
